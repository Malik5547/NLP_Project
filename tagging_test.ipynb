{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m medical_texts_train, predictions_train \u001b[38;5;241m=\u001b[39m extract_medical_texts_and_predictions(train_dataset)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# medical_texts_test, predictions_test = extract_medical_texts_and_predictions(test_dataset)\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmedical_texts_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(extract_features(sentences[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(extract_word_positions(medical_texts_train[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(base_text)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(base_text):\n\u001b[0;32m     48\u001b[0m     text \u001b[38;5;241m=\u001b[39m remove_useless_symbols(base_text)\n\u001b[1;32m---> 49\u001b[0m     lang \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes_core_news_md\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mca_core_news_md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mdetect_language\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_language\u001b[39m(text):\n\u001b[0;32m     22\u001b[0m     translator \u001b[38;5;241m=\u001b[39m Translator()\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlang\n",
      "File \u001b[1;32me:\\Apps\\Python\\Lib\\site-packages\\googletrans\\client.py:255\u001b[0m, in \u001b[0;36mTranslator.detect\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(lang)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 255\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# actual source language that will be recognized by Google Translator when the\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# src passed is equal to auto.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32me:\\Apps\\Python\\Lib\\site-packages\\googletrans\\client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, dest, src, override):\n\u001b[1;32m---> 78\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[0;32m     80\u001b[0m                                 token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[0;32m     82\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[1;32me:\\Apps\\Python\\Lib\\site-packages\\googletrans\\gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[1;32me:\\Apps\\Python\\Lib\\site-packages\\googletrans\\gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import spacy\n",
    "\n",
    "\n",
    "def extract_medical_texts_and_predictions(dataset):\n",
    "    medical_texts = []\n",
    "    predictions = []\n",
    "\n",
    "    for data_element in dataset:\n",
    "        medical_text = data_element['data']['text']\n",
    "        medical_texts.append(medical_text)\n",
    "\n",
    "        for result_element in data_element['predictions']:\n",
    "            predictions.append(result_element['result'])\n",
    "\n",
    "    return medical_texts, predictions\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    translator = Translator()\n",
    "\n",
    "    return translator.detect(text).lang\n",
    "\n",
    "\n",
    "def remove_useless_symbols(text):\n",
    "    allowed_punctuation = r'\\.,;:\\\"!'\n",
    "    pattern = f'[^{allowed_punctuation}\\\\w\\\\s]'\n",
    "    return re.sub(pattern, '', text).strip()\n",
    "\n",
    "\n",
    "def extract_word_positions(text):\n",
    "    pattern = re.compile(r'\\w+|[^\\w\\s]')\n",
    "    matches = pattern.finditer(text)\n",
    "\n",
    "    words_with_indices = {match.group(): {'start': match.start(), 'end': match.end()} for match in matches}\n",
    "\n",
    "    return words_with_indices\n",
    "\n",
    "\n",
    "def is_useless(word):\n",
    "    pattern = re.compile(r\"[a-zA-Z]|\\d\")\n",
    "    return not pattern.search(word)\n",
    "\n",
    "\n",
    "def preprocess_text(base_text):\n",
    "    text = remove_useless_symbols(base_text)\n",
    "    lang = detect_language(text)\n",
    "\n",
    "    nlp = spacy.load('es_core_news_md') if lang == 'es' else spacy.load('ca_core_news_md')\n",
    "\n",
    "    doc = nlp(text)\n",
    "    print(f'Text is :{text}')\n",
    "    lemmas_per_sentence = [[token for token in sentence if not is_useless(token.text)] for sentence in doc.sents]\n",
    "    print(f'Token per sentence are:{lemmas_per_sentence}')\n",
    "\n",
    "    return lemmas_per_sentence\n",
    "\n",
    "\n",
    "def extract_features(sentence, i):\n",
    "    token = sentence[i]\n",
    "    word = token.text\n",
    "\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word_lower': word.lower(),\n",
    "        'is_capitalized': word[0].isupper(),\n",
    "        'is_all_caps': word.isupper(),\n",
    "        'is_digit': word.isdigit(),\n",
    "        'word_length': len(word),\n",
    "        'contains_digits': bool(re.search(r'\\d', word)),\n",
    "        'pos': token.pos_,\n",
    "        'lemma': token.lemma_\n",
    "    }\n",
    "\n",
    "    features[\"prefix_2\"] = word[:2]\n",
    "    features[\"suffix_2\"] = word[-2:]\n",
    "\n",
    "    if i > 0:\n",
    "        previous_tokens = [sentence[j - 1].text.lower() for j in range(max(0, i - 6), i)]\n",
    "        features[\"previous_words\"] = previous_tokens\n",
    "\n",
    "    if i < len(sentence) - 1:\n",
    "        next_token = sentence[i + 1].text\n",
    "        features[\"next_word\"] = next_token.lower()\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "with open('train_data.json', 'r', encoding='utf-8') as train_file:\n",
    "    train_dataset = json.load(train_file)\n",
    "\n",
    "# with open('test.json', 'r', encoding='utf-8') as test_file:\n",
    "#     test_dataset = json.load(test_file)\n",
    "\n",
    "medical_texts_train, predictions_train = extract_medical_texts_and_predictions(train_dataset)\n",
    "# medical_texts_test, predictions_test = extract_medical_texts_and_predictions(test_dataset)\n",
    "\n",
    "sentences = preprocess_text(medical_texts_train[0])\n",
    "print(extract_features(sentences[0], 1))\n",
    "print(extract_word_positions(medical_texts_train[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

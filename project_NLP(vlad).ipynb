{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook we are using the Rule Based appraoch to solve the Negation & Uncertainty identification problem for a medical based corpus.\n",
        "Our approach will revolve around working with the regex library \"re\" and the various methods provided by it.\n",
        "The \"json\" library is used for parsing the train and tst files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8V6veP5-ByqK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2GF2GPXAxXi"
      },
      "source": [
        "Parse Training and Testing data from the JSON files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nJJ1HEr2147_"
      },
      "outputs": [],
      "source": [
        "def parse_json(file_path):\n",
        "\n",
        "  # Step 1: Open the file in read mode\n",
        "  try:\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "      # Step 2: Load the JSON data using json.load()\n",
        "      parsed_file = json.load(json_file)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  else:\n",
        "    print(\"JSON data parsed successfully!\")\n",
        "    # Step 3: Access and process the data\n",
        "  return parsed_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QPozyd5xRM",
        "outputId": "07f57a46-17cf-4fba-96e5-52b4911d8e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data parsed successfully!\n",
            "JSON data parsed successfully!\n"
          ]
        }
      ],
      "source": [
        "training_set = parse_json(\"./train_data.json\")\n",
        "testing_set = parse_json(\"./test_data.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The repartition of data is split following a 80:20 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vgfvxL2s5P",
        "outputId": "825e6b3f-a93e-4ae2-bb75-495c6d1fb73f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "254\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "print(len(training_set))\n",
        "print(len(testing_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the training set, extract the GT predictions and the texts.\n",
        "From the test set, extract the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "rDlG9kmj5xRO"
      },
      "outputs": [],
      "source": [
        "predictions = [document[\"predictions\"] for document in training_set]\n",
        "texts = [document[\"data\"][\"text\"] for document in training_set]\n",
        "test_texts = [document[\"data\"][\"text\"] for document in testing_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO47cFyb30E-"
      },
      "source": [
        "We are going to create a vocabulary of words that will enable us to create a Rule Based Model.\n",
        "Firstly, we will use CUTEXT, which is a tool that provides popular medical terms in Spanish.\n",
        "All of the words contained in CUTEXT will be added to our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DEnc1hI2-dcN"
      },
      "outputs": [],
      "source": [
        "def extract_terms_from_file(file_path):\n",
        "    terms = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"Term:\"):\n",
        "                term = line.split(\"Term:\")[1].strip()\n",
        "                terms.append(term)\n",
        "    return terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filtering unnecessary words, such as those that contain punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oiUsBmvlK7BF"
      },
      "outputs": [],
      "source": [
        "def parse_terms(extracted_terms):\n",
        "  new_terms = []\n",
        "  for term in extracted_terms:\n",
        "    if term[0].isalpha() and term[-1].isalpha() and \"**\" not in term and \"(\" not in term and \")\" not in term and len(term)>3:\n",
        "      new_terms.append(term)\n",
        "  return new_terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Ur92zYTcItiX"
      },
      "outputs": [],
      "source": [
        "file_path = \"./terms_raw.txt\"\n",
        "# Extract terms from the file\n",
        "cutext_terms = extract_terms_from_file(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOcMyguOI351",
        "outputId": "6e91fb16-4e07-4b8f-a9a2-5b583b619ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cutext terms before filtering:  21554\n"
          ]
        }
      ],
      "source": [
        "print(\"Cutext terms before filtering: \", len(cutext_terms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "kk3JZb87Mt83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cutext terms after filtering:  18236\n"
          ]
        }
      ],
      "source": [
        "cutext_terms = parse_terms(cutext_terms)\n",
        "print(\"Cutext terms after filtering: \", len(cutext_terms))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKPRePm5xRO"
      },
      "source": [
        "Extract NEG, UNC, NSCO and USCO from Training Data annotations (Ground-Truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "CgMZeYPB5xRO"
      },
      "outputs": [],
      "source": [
        "# Gets a list of tuples representing character offsets (start, end) and returns list of words mapped from the text\n",
        "def get_words(text, offsets):\n",
        "  words = []\n",
        "  for start, end in offsets:\n",
        "    if text[start-1].isalpha():\n",
        "      s=start-1\n",
        "    else:\n",
        "      s=start\n",
        "    if text[end-1].isalpha():\n",
        "      e=end\n",
        "    else:\n",
        "      e=end-1\n",
        "    words.append(text[s:e])\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "NNwlbJFT5xRP"
      },
      "outputs": [],
      "source": [
        "# Parses a document and returns 4 lists representing words for each category\n",
        "def find_cues_and_scopes(document):\n",
        "  neg_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NEG\" in result_element[\"value\"][\"labels\"]]\n",
        "  unc_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"UNC\" in result_element[\"value\"][\"labels\"]]\n",
        "  nsco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NSCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  usco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"USCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  neg_words = get_words(document[\"data\"][\"text\"], neg_postitions_pairs)\n",
        "  unc_words = get_words(document[\"data\"][\"text\"], unc_postitions_pairs)\n",
        "  nsco_words = get_words(document[\"data\"][\"text\"], nsco_postitions_pairs)\n",
        "  usco_words = get_words(document[\"data\"][\"text\"], usco_postitions_pairs)\n",
        "  return neg_words, unc_words, nsco_words, usco_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3KCH-BsM5xRP"
      },
      "outputs": [],
      "source": [
        "NEG = set()\n",
        "UNC = set()\n",
        "NSCO = set()\n",
        "USCO = set()\n",
        "\n",
        "for document in training_set:\n",
        "  neg_words, unc_words, nsco_words, usco_words = find_cues_and_scopes(document)\n",
        "  nsco_words_set = set(nsco_words)\n",
        "  usco_words_set = set(usco_words)\n",
        "\n",
        "  NEG.update(neg_words)\n",
        "  UNC.update(unc_words)\n",
        "  NSCO.update(nsco_words)\n",
        "  USCO.update(usco_words)\n",
        "\n",
        "# Removing spaces and punctation signs from the start and end of each string\n",
        "NEG = {word.strip(\" ,.!?;)\") for word in NEG}\n",
        "UNC = {word.strip(\" ,.!?);\") for word in UNC}\n",
        "NSCO = {word.strip(\" ,.!?;)\") for word in NSCO}\n",
        "USCO = {word.strip(\" ,.!?);\") for word in USCO}\n",
        "\n",
        "# Some negation cues from NEG are also found as UNCs a small amount of times.\n",
        "# To avoid labeling one word as both UNC and NEG, we will remove words from UNC that are also present in NEG,\n",
        "# mainly because 90% of the times those words are NEG in the GTs.\n",
        "# Remove negation from UNC\n",
        "for word in NEG:\n",
        "  if word in UNC:\n",
        "    UNC.remove(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjomeTlu8oHN"
      },
      "source": [
        "Now we created a vocabulary for all the 4 categories of words.\n",
        "\n",
        "NEG and UNC should be different, however there is no reason for applying the same rule for NSCO and USCO, and we will explain why.\n",
        "\n",
        "Our search for negations is going to start by using a regex formed by concatenating all the words from NEG in order to look for the negation cues.\n",
        "\n",
        "Next, we will analyze 5 words before and 5 words after that cue looking for its scope.\n",
        "\n",
        "Same algorithm will be applied for uncertainties.\n",
        "\n",
        "Therefore, it is impossible to match a NEG with USCO or an UNC with a NSCO. This means we can comfortably combine all the scopes into one big vocabulary to achieve greater coverage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtv4WScpWxrl",
        "outputId": "46f88ac2-e300-4d50-a1e2-1173e8eadf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALL_SCOPES size:  2617\n",
            "Scopes words before processing:  3184\n",
            "Scopes words after processing:  2895\n"
          ]
        }
      ],
      "source": [
        "ALL_SCOPES = NSCO.union(USCO)\n",
        "\n",
        "print(\"ALL_SCOPES size: \",  len(ALL_SCOPES))\n",
        "\n",
        "# A set with all individual words from the scopes\n",
        "SCOPE_words = set()         # ['erc', '(29/05/18)', 'ser', 'visibles', 'extratono', 'inicia', 'valor', 'frialdad', 'medicamentoses', 'neoformativo']\n",
        "for scope in ALL_SCOPES:\n",
        "  SCOPE_words.update(scope.split())\n",
        "\n",
        "print(\"Scopes words before processing: \", len(SCOPE_words))\n",
        "\n",
        "# Remove all symbols and numbers from the set\n",
        "SCOPE_words = {word for word in SCOPE_words if word.isalpha()}\n",
        "SCOPE_words = list(SCOPE_words)\n",
        "\n",
        "\n",
        "print(\"Scopes words after processing: \", len(SCOPE_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28sA1kjp9D76"
      },
      "source": [
        "Combine SCOPE_words with extracted_terms from CUTEXT to achieve even freater coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cTOnXwCaYtMx"
      },
      "outputs": [],
      "source": [
        "extracted_terms = list(set(cutext_terms+SCOPE_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKDyK0dRY929",
        "outputId": "fda7ead0-ba72-40ff-ba1a-1ef146866cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total size of medical scopes terms:  19948\n"
          ]
        }
      ],
      "source": [
        "print(\"Total size of medical scopes terms: \", len(extracted_terms))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the vocabulary has been created and the next step is to generate a regex using it.\n",
        "\n",
        "Our aim is to match as much as possible from matched sequence, hence we will place the scopes in the regex in a descending order regarding their length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "_vplFWHbEprg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['intervencion quirurgica de retirada de material de osteosintesis', 'desgarros puerperio procedimientos venoclisis monitorizacion nst', 'per metapneumovirus procediments aspirat nasofaringi tractament', 'iq antecedents antecedents patologic asma intermitent lleu', 'lesiones de caracteristicas inflamatorio-desmielinizantes', \"anys procedencia aguts servei obstetricia data d'ingres\", \"anys procedencia aguts servei traumatics data d'ingres\", \"mateix hosp servei reconstruc osteoartic data d'ingres\", 'genoll dret antecedents antecedents patologic diabetis', \"anys procedencia aguts servei nefrologia data d'ingres\"]\n"
          ]
        }
      ],
      "source": [
        "extracted_terms.sort(key=len,reverse=True)\n",
        "print(extracted_terms[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeXd2xOB_OOE"
      },
      "source": [
        "Prepare REGEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "TIKVWGYf3fv1"
      },
      "outputs": [],
      "source": [
        "NEG_pattern = \"|\".join(NEG)\n",
        "UNC_pattern = \"|\".join(UNC)\n",
        "SCOPE_pattern = \"|\".join(extracted_terms)\n",
        "SCOPE_pattern_baseline = \"|\".join(SCOPE_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "cxmKcN5VCYZk"
      },
      "outputs": [],
      "source": [
        "# SCOPE pattern just for CUTEXT.\n",
        "SCOPE_pattern_CUTEXT = \"|\".join(cutext_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "PgvAtD98vpxh"
      },
      "outputs": [],
      "source": [
        "# Regex for identifying the scopes that appear before the cues.\n",
        "regex_neg_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\b({NEG_pattern})\\b\"\n",
        "regex_unc_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\b({UNC_pattern})\\b\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clIK5DtE2ZDp"
      },
      "source": [
        "REGEX Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "KZSj1OYg2X0D"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern_baseline})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre =rf\"\\b({UNC_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern_baseline})\\b\\s*){{0,5}})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy02iJ7Nqz6X"
      },
      "source": [
        "REGEX CUTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "F1bN9VlSqtiv"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,5}})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qylXAB6qp2pN"
      },
      "source": [
        "REGEX1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "ZvOrpfG57VM3"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upon qualitative analysis of the Ground Truths, we came to the conclusion that a regex that takes all the words after the cue until the end of sentence as a scope is a viable approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REGEX that takes all the words until the end of the proposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex_neg_pre = rf\"\\b({NEG_pattern})\\b\\s*(.*?)\\.\"\n",
        "regex_unc_pre = rf\"\\b({UNC_pattern})\\b\\s*(.*?)\\.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5piE7K3VqNZ",
        "outputId": "7bc430b3-d83f-4a9f-fa32-de95b00b04fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "323583\n"
          ]
        }
      ],
      "source": [
        "print(len(SCOPE_pattern))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U66u6gH3_aM0"
      },
      "source": [
        "Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "ERlHYxLZvIHG"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for i in range(len(test_texts)):\n",
        "  dict = {\"NEG\":set(),\"NSCO\":set(),\"UNC\":set(),\"USCO\":set()}\n",
        "\n",
        "  predictions.append(dict)\n",
        "\n",
        "\n",
        "for id, test_text in enumerate(test_texts):\n",
        "  neg_scopes_pre_matches = re.finditer(regex_neg_pre, test_text)\n",
        "  neg_scopes_pos_matches = re.finditer(regex_neg_pos, test_text)\n",
        "  unc_scopes_pre_matches = re.finditer(regex_unc_pre, test_text)\n",
        "  unc_scopes_pos_matches = re.finditer(regex_unc_pos, test_text)\n",
        "\n",
        "  if neg_scopes_pre_matches:\n",
        "    for match in neg_scopes_pre_matches:\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start(1)\n",
        "        end_pos = match.end(1)+1\n",
        "\n",
        "        predictions[id][\"NEG\"].add((start_pos,end_pos,matched_word))\n",
        "\n",
        "        # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "\n",
        "        predictions[id][\"NSCO\"].add((sc_start_pos,sc_end_pos,scope_word))\n",
        "\n",
        "  if unc_scopes_pre_matches:\n",
        "    for match in unc_scopes_pre_matches:\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end(1)+1\n",
        "\n",
        "        predictions[id][\"UNC\"].add((start_pos,end_pos,matched_word))\n",
        "\n",
        "\n",
        "        # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "\n",
        "        predictions[id][\"USCO\"].add((sc_start_pos,sc_end_pos,scope_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIXFSiPV_n-5"
      },
      "source": [
        "Sort the text predictions by starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "kP9PXuiBy5xA"
      },
      "outputs": [],
      "source": [
        "for dict in predictions:\n",
        "    for key,value in dict.items():\n",
        "\n",
        "      sorted_value=sorted(list(value), key=lambda x: x[0])\n",
        "      dict[key] = sorted_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20kWSstUYBox",
        "outputId": "c3ff1761-8cf1-4241-d278-eff5cee2255c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': [(395, 398, 'no'), (499, 505, 'niega'), (1141, 1144, 'no'), (1163, 1166, 'no'), (1313, 1322, 'negativo'), (2118, 2122, 'sin')], 'NSCO': [(398, 433, 'alergias medicamentosas conocidas '), (505, 541, 'habitos toxicos medicacio habitual '), (1144, 1151, 'inmune'), (1166, 1173, 'immune'), (1322, 1323, ''), (2122, 2134, 'incidencias')], 'UNC': [(3460, 3466, 'puede')], 'USCO': [(3466, 3467, '')]}\n"
          ]
        }
      ],
      "source": [
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tKQTkf9_xsz"
      },
      "source": [
        "Get ground thruth from testing_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "nsXxzCX3FXB7"
      },
      "outputs": [],
      "source": [
        "def get_gt_format(document):\n",
        "    neg_predictions, unc_predictions, nsco_predictions, usco_predictions = [], [], [], []\n",
        "    text = document[\"data\"][\"text\"]\n",
        "    for result_element in document[\"predictions\"][0][\"result\"]:\n",
        "        start = result_element[\"value\"][\"start\"]\n",
        "        end = result_element[\"value\"][\"end\"]\n",
        "        if \"NEG\" in result_element[\"value\"][\"labels\"]:\n",
        "            neg_predictions.append((start, end, text[start:end]))\n",
        "        if \"UNC\" in result_element[\"value\"][\"labels\"]:\n",
        "            unc_predictions.append((start, end, text[start:end]))\n",
        "        if \"NSCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            nsco_predictions.append((start, end, text[start:end]))\n",
        "        if \"USCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            usco_predictions.append((start, end, text[start:end]))\n",
        "\n",
        "    return neg_predictions, unc_predictions, nsco_predictions, usco_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-QWt5ZE3-D",
        "outputId": "0e0e424d-73f4-40b2-b5b4-870ad9cd327b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'NEG': [(395, 398, 'no '),\n",
              "  (499, 505, 'niega '),\n",
              "  (1111, 1119, 'negativo'),\n",
              "  (1141, 1144, 'no '),\n",
              "  (1163, 1166, 'no '),\n",
              "  (1194, 1203, 'negativos'),\n",
              "  (2118, 2122, 'sin ')],\n",
              " 'UNC': [],\n",
              " 'NSCO': [(398, 422, 'alergias medicamentosas '),\n",
              "  (505, 521, 'habitos toxicos '),\n",
              "  (1107, 1111, 'vih '),\n",
              "  (1144, 1150, 'inmune'),\n",
              "  (1166, 1172, 'immune'),\n",
              "  (1174, 1194, 'lues vih, vhb y vhc '),\n",
              "  (2122, 2133, 'incidencias')],\n",
              " 'USCO': []}"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FORMAT : (NEG, START, END, WORD)\n",
        "def get_ground_truth(document):\n",
        "    neg_results, unc_results, nsco_results, usco_results = get_gt_format(document)\n",
        "\n",
        "    neg_results_sorted = sorted(neg_results, key=lambda x: x[0])\n",
        "    unc_results_sorted = sorted(unc_results, key=lambda x: x[0])\n",
        "    nsco_results_sorted = sorted(nsco_results, key=lambda x: x[0])\n",
        "    usco_results_sorted = sorted(usco_results, key=lambda x: x[0])\n",
        "\n",
        "    ground_truth_dict = {\"NEG\": neg_results_sorted, \"UNC\": unc_results_sorted, \"NSCO\": nsco_results_sorted, \"USCO\": usco_results_sorted}\n",
        "\n",
        "    return ground_truth_dict\n",
        "\n",
        "get_ground_truth(testing_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "a9yh7F-jE6tU"
      },
      "outputs": [],
      "source": [
        "# List of dictionaries of GT docuemnts in the test set\n",
        "ground_truths = [get_ground_truth(document) for document in testing_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh9oGSB2Bcii"
      },
      "source": [
        "Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "x8dBo8oiF8Ho"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions,ground_truths):\n",
        "  precision = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  recall = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  f1 = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  tp = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_predictions = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_ground_truths = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  for d1,d2 in zip(predictions,ground_truths):\n",
        "\n",
        "    for key in d1:\n",
        "      for elem in d1[key]:\n",
        "        for elem2 in d2[key]:\n",
        "          # We are allowing an error of 1 character between GT and our predictions \n",
        "          # because the tagging is inconsistent in the GT and many times it happens that the punctuation sign or space\n",
        "          # are included in the scope. \n",
        "          if abs(elem[0]-elem2[0]) <= 1 and abs(elem[1]-elem2[1]) <=1:\n",
        "            tp[key]+=1\n",
        "            break\n",
        "\n",
        "      num_of_predictions[key]+=len(d1[key])\n",
        "      num_of_ground_truths[key]+=len(d2[key])\n",
        "\n",
        "  for key in precision:\n",
        "    precision[key] = tp[key]/num_of_predictions[key]\n",
        "    recall[key] = tp[key]/num_of_ground_truths[key]\n",
        "    f1[key] = 2*precision[key]*recall[key]/(precision[key]+recall[key])\n",
        "\n",
        "\n",
        "  return precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "ugryzw7-z1T6"
      },
      "outputs": [],
      "source": [
        "precision, recall, f1 = calculate_metrics(predictions,ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmuaHq_92t7e"
      },
      "source": [
        "Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKJflAwl2wpl",
        "outputId": "77c7d3e8-2fdc-4271-a4a3-821e33367790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9088345864661654, 'NSCO': 0.5281954887218046, 'UNC': 0.5662650602409639, 'USCO': 0.14457831325301204}\n",
            "{'NEG': 0.8542402826855123, 'NSCO': 0.5232774674115456, 'UNC': 0.7175572519083969, 'USCO': 0.18604651162790697}\n",
            "{'NEG': 0.8806921675774135, 'NSCO': 0.5257249766136575, 'UNC': 0.632996632996633, 'USCO': 0.16271186440677965}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiqTWLBOGVkt"
      },
      "source": [
        "CUTEXT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvBibqG3Ga_9",
        "outputId": "e289e0c7-ca67-406e-80d1-4dc72cf1f3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9058713886300093, 'NSCO': 0.16775396085740912, 'UNC': 0.5562130177514792, 'USCO': 0.07692307692307693}\n",
            "{'NEG': 0.8586572438162544, 'NSCO': 0.16759776536312848, 'UNC': 0.7175572519083969, 'USCO': 0.10077519379844961}\n",
            "{'NEG': 0.8816326530612244, 'NSCO': 0.16767582673497902, 'UNC': 0.6266666666666667, 'USCO': 0.08724832214765099}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vm3g-nDobLJ"
      },
      "source": [
        "CUTEXT + Scope_words REGEX 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-9fgOyLAJJe",
        "outputId": "0d77cce8-b0a7-4d99-87d9-d112c49b5705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9082308420056765, 'NSCO': 0.5723746452223274, 'UNC': 0.5662650602409639, 'USCO': 0.21084337349397592}\n",
            "{'NEG': 0.8480565371024735, 'NSCO': 0.5633147113594041, 'UNC': 0.7175572519083969, 'USCO': 0.2713178294573643}\n",
            "{'NEG': 0.8771128369118318, 'NSCO': 0.5678085405912718, 'UNC': 0.632996632996633, 'USCO': 0.23728813559322035}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Until the end of the proposition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9134615384615384, 'NSCO': 0.55, 'UNC': 0.5253164556962026, 'USCO': 0.2721518987341772}\n",
            "{'NEG': 0.8392226148409894, 'NSCO': 0.5325884543761639, 'UNC': 0.6335877862595419, 'USCO': 0.3333333333333333}\n",
            "{'NEG': 0.874769797421731, 'NSCO': 0.5411542100283822, 'UNC': 0.5743944636678201, 'USCO': 0.29965156794425085}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LkOktoy5xRL"
      },
      "source": [
        "training_set = Tot setul de date, este vectorul care contine toate documentele\n",
        "document = Element din training_set. Fiecare document contine \"data\", \"annotations\" si \"predictions\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "8V6veP5-ByqK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2GF2GPXAxXi"
      },
      "source": [
        "Parse Training and Testing data from JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "nJJ1HEr2147_"
      },
      "outputs": [],
      "source": [
        "def parse_json(file_path):\n",
        "\n",
        "  # Step 2: Open the file in read mode\n",
        "  try:\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "      # Step 3: Load the JSON data using json.load()\n",
        "      parsed_file = json.load(json_file)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  else:\n",
        "    print(\"JSON data parsed successfully!\")\n",
        "    # Step 4: Access and process the data\n",
        "    # (See examples below based on data structure)\n",
        "  return parsed_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QPozyd5xRM",
        "outputId": "24f172dd-500f-4275-9bc4-fc486173abab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data parsed successfully!\n",
            "JSON data parsed successfully!\n"
          ]
        }
      ],
      "source": [
        "training_set = parse_json(\"./train_data.json\")\n",
        "testing_set = parse_json(\"./test_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vgfvxL2s5P",
        "outputId": "efb361e6-ccd9-4482-f698-8e015209d9fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "254\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "print(len(training_set))\n",
        "print(len(testing_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "rDlG9kmj5xRO"
      },
      "outputs": [],
      "source": [
        "predictions = [document[\"predictions\"] for document in training_set]\n",
        "texts = [document[\"data\"][\"text\"] for document in training_set]\n",
        "test_texts = [document[\"data\"][\"text\"] for document in testing_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO47cFyb30E-"
      },
      "source": [
        "Extract terms given by CUTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "DEnc1hI2-dcN"
      },
      "outputs": [],
      "source": [
        "def extract_terms_from_file(file_path):\n",
        "    terms = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"Term:\"):\n",
        "                term = line.split(\"Term:\")[1].strip()\n",
        "                terms.append(term)\n",
        "    return terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "oiUsBmvlK7BF"
      },
      "outputs": [],
      "source": [
        "def parse_terms(extracted_terms):\n",
        "  new_terms = []\n",
        "  for term in extracted_terms:\n",
        "    if term[0].isalpha() and term[-1].isalpha() and \"**\" not in term and \"(\" not in term and \")\" not in term and len(term)>3:\n",
        "      new_terms.append(term)\n",
        "  return new_terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Ur92zYTcItiX"
      },
      "outputs": [],
      "source": [
        "file_path = \"./terms_raw.txt\"\n",
        "# Extract terms from the file\n",
        "cutext_terms = extract_terms_from_file(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOcMyguOI351",
        "outputId": "de4ad4ff-0535-40d1-aae8-628c03fcbc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21554\n"
          ]
        }
      ],
      "source": [
        "print(len(cutext_terms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "kk3JZb87Mt83"
      },
      "outputs": [],
      "source": [
        "cutext_terms = parse_terms(cutext_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKPRePm5xRO"
      },
      "source": [
        "Extract NEG, UNC, NSCO and USCO from Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "CgMZeYPB5xRO"
      },
      "outputs": [],
      "source": [
        "# Gets a list of tuples representing character offsets and returns list of words\n",
        "def get_words(text, offsets):\n",
        "  words = []\n",
        "  for start, end in offsets:\n",
        "    #words.append(text[start:end-1])\n",
        "    #words.append(text[start-1:end])\n",
        "    if text[start-1].isalpha():\n",
        "      s=start-1\n",
        "    else:\n",
        "      s=start\n",
        "    if text[end-1].isalpha():\n",
        "      e=end\n",
        "    else:\n",
        "      e=end-1\n",
        "    words.append(text[s:e])\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "NNwlbJFT5xRP"
      },
      "outputs": [],
      "source": [
        "# Parses a document and returns 4 lists of tuples representing words\n",
        "def find_cues_and_scopes(document):\n",
        "  neg_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NEG\" in result_element[\"value\"][\"labels\"]]\n",
        "  unc_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"UNC\" in result_element[\"value\"][\"labels\"]]\n",
        "  nsco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NSCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  usco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"USCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  neg_words = get_words(document[\"data\"][\"text\"], neg_postitions_pairs)\n",
        "  unc_words = get_words(document[\"data\"][\"text\"], unc_postitions_pairs)\n",
        "  nsco_words = get_words(document[\"data\"][\"text\"], nsco_postitions_pairs)\n",
        "  usco_words = get_words(document[\"data\"][\"text\"], usco_postitions_pairs)\n",
        "  return neg_words, unc_words, nsco_words, usco_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_cue_sets(document):\n",
        "\n",
        "  neg_pre_cues = []\n",
        "  neg_post_cues = []\n",
        "  unc_pre_cues = []\n",
        "  unc_post_cues = []\n",
        "\n",
        "  neg_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NEG\" in result_element[\"value\"][\"labels\"]]\n",
        "  unc_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"UNC\" in result_element[\"value\"][\"labels\"]]\n",
        "  nsco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NSCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  usco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"USCO\" in result_element[\"value\"][\"labels\"]]\n",
        "\n",
        "\n",
        "  neg_words = get_words(document[\"data\"][\"text\"], neg_postitions_pairs)\n",
        "  unc_words = get_words(document[\"data\"][\"text\"], unc_postitions_pairs)\n",
        "  nsco_words = get_words(document[\"data\"][\"text\"], nsco_postitions_pairs)\n",
        "  usco_words = get_words(document[\"data\"][\"text\"], usco_postitions_pairs)\n",
        "\n",
        "\n",
        "  neg_tpl = [(neg_words[i], neg_postitions_pairs[i][0], neg_postitions_pairs[i][1]) for i in range(len(neg_words))]\n",
        "  unc_tpl = [(unc_words[i], unc_postitions_pairs[i][0], unc_postitions_pairs[i][1]) for i in range(len(unc_words))]\n",
        "  nsco_tpl = [(nsco_words[i], nsco_postitions_pairs[i][0], nsco_postitions_pairs[i][1]) for i in range(len(nsco_words))]\n",
        "  usco_tpl = [(usco_words[i], usco_postitions_pairs[i][0], usco_postitions_pairs[i][1]) for i in range(len(usco_words))]\n",
        "  \n",
        "  neg_tpl = sorted(neg_tpl, key=lambda x: x[1])\n",
        "  unc_tpl = sorted(unc_tpl, key=lambda x: x[1])\n",
        "  nsco_tpl = sorted(nsco_tpl, key=lambda x: x[1])\n",
        "  usco_tpl = sorted(usco_tpl, key=lambda x: x[1])\n",
        "\n",
        "  for i,j in zip(range(len(neg_tpl)), range(len(nsco_tpl))):\n",
        "    if neg_tpl[i][1] - nsco_tpl[j][2] > 10:\n",
        "        i -= 1\n",
        "        continue\n",
        "    elif nsco_tpl[j][1] - neg_tpl[i][2] > 10:\n",
        "        j -= 1\n",
        "        continue\n",
        "    if neg_tpl[i][1] > nsco_tpl[i][1]:\n",
        "      neg_post_cues.append(neg_tpl[i][0])\n",
        "    else:\n",
        "      neg_pre_cues.append(neg_tpl[i][0])\n",
        "\n",
        "  for i,j in zip(range(len(unc_tpl)), range(len(usco_tpl))):\n",
        "    if unc_tpl[i][1] - usco_tpl[j][2] > 10:\n",
        "        i -= 1\n",
        "        continue\n",
        "    elif usco_tpl[j][1] - unc_tpl[i][2] > 10:\n",
        "        j -= 1\n",
        "        continue\n",
        "    if unc_tpl[i][1] > usco_tpl[i][1]:\n",
        "      unc_post_cues.append(unc_tpl[i][0])\n",
        "    else:\n",
        "      unc_pre_cues.append(unc_tpl[i][0])\n",
        "\n",
        "  return neg_pre_cues, neg_post_cues, unc_pre_cues, unc_post_cues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# pre = apar inainte de scope\n",
        "neg_pre_cues = set()\n",
        "neg_post_cues = set()\n",
        "unc_pre_cues = set()\n",
        "unc_post_cues = set()\n",
        "\n",
        "neg_pre_dict = {}\n",
        "neg_post_dict = {}\n",
        "unc_pre_dict = {}\n",
        "unc_post_dict = {}\n",
        "\n",
        "for document in training_set:\n",
        "    neg_pre, neg_post, unc_pre, unc_post = create_cue_sets(document)\n",
        "    for word in neg_pre:\n",
        "        if word in neg_pre_dict:\n",
        "            neg_pre_dict[word] += 1\n",
        "        else:\n",
        "            neg_pre_dict[word] = 1\n",
        "    for word in neg_post:\n",
        "        if word in neg_post_dict:\n",
        "            neg_post_dict[word] += 1\n",
        "        else:\n",
        "            neg_post_dict[word] = 1\n",
        "    for word in unc_pre:\n",
        "        if word in unc_pre_dict:\n",
        "            unc_pre_dict[word] += 1\n",
        "        else:\n",
        "            unc_pre_dict[word] = 1\n",
        "    for word in unc_post:\n",
        "        if word in unc_post_dict:\n",
        "            unc_post_dict[word] += 1\n",
        "        else:\n",
        "            unc_post_dict[word] = 1\n",
        "    neg_pre_cues.update(neg_pre)\n",
        "    neg_post_cues.update(neg_post)\n",
        "    unc_pre_cues.update(unc_pre)\n",
        "    unc_post_cues.update(unc_post)\n",
        "\n",
        "\n",
        "neg_pre_blacklist = set()\n",
        "neg_post_blacklist = set()\n",
        "unc_pre_blacklist = set()\n",
        "unc_post_blacklist = set()\n",
        "\n",
        "for word in neg_pre_cues:\n",
        "    if word in neg_post_cues:\n",
        "        if neg_pre_dict[word] > neg_post_dict[word]:\n",
        "            neg_post_blacklist.add(word)\n",
        "        else:\n",
        "            neg_pre_blacklist.add(word)\n",
        "\n",
        "for word in unc_pre_cues:\n",
        "    if word in unc_post_cues:\n",
        "        if unc_pre_dict[word] > unc_post_dict[word]:\n",
        "            unc_post_blacklist.add(word)\n",
        "        else:\n",
        "            unc_pre_blacklist.add(word)\n",
        "\n",
        "neg_pre_cues = neg_pre_cues - neg_pre_blacklist\n",
        "neg_post_cues = neg_post_cues - neg_post_blacklist\n",
        "unc_pre_cues = unc_pre_cues - unc_pre_blacklist\n",
        "unc_post_cues = unc_post_cues - unc_post_blacklist\n",
        "\n",
        "print(\"no\" in  neg_pre_cues)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "for neg in neg_pre_cues:\n",
        "  if neg in neg_post_cues:\n",
        "    print(\"E in ambele:\", neg)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "3KCH-BsM5xRP"
      },
      "outputs": [],
      "source": [
        "NEG = set()\n",
        "UNC = set()\n",
        "NSCO = set()\n",
        "USCO = set()\n",
        "\n",
        "num_nsco = 0\n",
        "num_usco = 0\n",
        "for document in training_set:\n",
        "  neg_words, unc_words, nsco_words, usco_words = find_cues_and_scopes(document)\n",
        "  nsco_words_set = set(nsco_words)\n",
        "  usco_words_set = set(usco_words)\n",
        "  num_nsco += len(nsco_words)\n",
        "  num_usco += len(usco_words)\n",
        "\n",
        "  NEG.update(neg_words)\n",
        "  UNC.update(unc_words)\n",
        "  NSCO.update(nsco_words)\n",
        "  USCO.update(usco_words)\n",
        "\n",
        "# Removing spaces and punctation signs from the start and end of each string\n",
        "NEG = {word.strip(\" ,.!?;)\") for word in NEG}\n",
        "UNC = {word.strip(\" ,.!?);\") for word in UNC}\n",
        "NSCO = {word.strip(\" ,.!?;)\") for word in NSCO}\n",
        "USCO = {word.strip(\" ,.!?);\") for word in USCO}\n",
        "\n",
        "# Remove negation from UNC\n",
        "for word in NEG:\n",
        "  if word in UNC:\n",
        "    UNC.remove(word)\n",
        "\n",
        "for word in NEG:\n",
        "  if word in unc_pre_cues:\n",
        "    unc_pre_cues.remove(word)\n",
        "  if word in unc_post_cues:\n",
        "    unc_post_cues.remove(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjomeTlu8oHN"
      },
      "source": [
        "Combine USCO and NSCO in SCOPE_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtv4WScpWxrl",
        "outputId": "46d62401-10c1-47ab-f3dd-30b7fd71905d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEG_UNC words before processing:  3184\n"
          ]
        }
      ],
      "source": [
        "ALL_SCOPES = NSCO.union(USCO)\n",
        "\n",
        "# A set with all individual words from the scopes\n",
        "SCOPE_words = set()         # ['erc', '(29/05/18)', 'ser', 'visibles', 'extratono', 'inicia', 'valor', 'frialdad', 'medicamentoses', 'neoformativo']\n",
        "for scope in ALL_SCOPES:\n",
        "  SCOPE_words.update(scope.split())\n",
        "\n",
        "print(\"NEG_UNC words before processing: \", len(SCOPE_words))\n",
        "\n",
        "# Remove all symbols and numbers from the set\n",
        "SCOPE_words = {word for word in SCOPE_words if word.isalpha()}\n",
        "SCOPE_words = list(SCOPE_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28sA1kjp9D76"
      },
      "source": [
        "Combine SCOPE_words with extracted_terms from CUTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "cTOnXwCaYtMx"
      },
      "outputs": [],
      "source": [
        "extracted_terms = list(set(cutext_terms+SCOPE_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKDyK0dRY929",
        "outputId": "9b8e8cfd-e3b9-4a0d-a4a9-827fe4c173e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19948\n"
          ]
        }
      ],
      "source": [
        "print(len(extracted_terms))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "_vplFWHbEprg"
      },
      "outputs": [],
      "source": [
        "extracted_terms.sort(key=len,reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeXd2xOB_OOE"
      },
      "source": [
        "Prepare REGEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "neg_pre_cues = {word.strip(\" *,.!?;)\") for word in neg_pre_cues}\n",
        "neg_post_cues = {word.strip(\" *,.!?;)\") for word in neg_post_cues}\n",
        "unc_pre_cues = {word.strip(\" *,.!?;)\") for word in unc_pre_cues}\n",
        "unc_post_cues = {word.strip(\" *,.!?;)\") for word in unc_post_cues}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEG_pre_pattern = \"|\".join(neg_pre_cues)\n",
        "UNC_pre_pattern = \"|\".join(unc_pre_cues)\n",
        "NEG_post_pattern = \"|\".join(neg_post_cues)\n",
        "UNC_post_pattern = \"|\".join(unc_post_cues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "TIKVWGYf3fv1"
      },
      "outputs": [],
      "source": [
        "NEG_pattern = \"|\".join(NEG)\n",
        "UNC_pattern = \"|\".join(UNC)\n",
        "#SCOPE pattern with CUTEXT + NSCO+USCO\n",
        "SCOPE_pattern = \"|\".join(extracted_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "cxmKcN5VCYZk"
      },
      "outputs": [],
      "source": [
        "#SCOPE pattern just for CUTEXT\n",
        "SCOPE_pattern_CUTEXT = \"|\".join(cutext_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "PgvAtD98vpxh"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre=rf\"\\b({NEG_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({SCOPE_pattern}\\b)\"\n",
        "regex_neg_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\b({NEG_pattern})\\b\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({SCOPE_pattern})\\b\"\n",
        "regex_unc_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({UNC_pattern})\\b\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "ZvOrpfG57VM3"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "e3pfd3RWEVPm"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre=rf\"\\b({NEG_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,7}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,7}})\"\n",
        "# make a regex that after finding a UNC patter, captures all the text until the first \".\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "8HpQRAAxLVR-"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pre_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pre_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "# Create a regex that firstly searches for the scope and then the negation. The scope can start maximum 5 words before the negation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex_neg_pos =rf\"({SCOPE_pattern})\\b\\s*({NEG_post_pattern})\"\n",
        "regex_unc_pos =rf\"({SCOPE_pattern})\\b\\s*({UNC_post_pattern})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5piE7K3VqNZ",
        "outputId": "8b6db2b6-ea5f-43ee-fc17-b6cdb8d2ff2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(regex_neg_pos[324000:324100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "regex_neg_pre = rf\"\\b({NEG_pre_pattern})\\b\\s*(.*?)\\.\"\n",
        "regex_unc_pre = rf\"\\b({UNC_pre_pattern})\\b\\s*(.*?)\\.\"\n",
        "regex_neg_pos =rf\"({SCOPE_pattern})\\b\\s*({NEG_post_pattern})\"\n",
        "regex_unc_pos =rf\"({SCOPE_pattern})\\b\\s*({UNC_post_pattern})\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U66u6gH3_aM0"
      },
      "source": [
        "Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "ERlHYxLZvIHG"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for i in range(len(test_texts)):\n",
        "  dict = {\"NEG\":set(),\"NSCO\":set(),\"UNC\":set(),\"USCO\":set()}\n",
        "  predictions.append(dict)\n",
        "\n",
        "\n",
        "for id, test_text in enumerate(test_texts):\n",
        "  neg_scopes_pre_matches = re.finditer(regex_neg_pre, test_text)\n",
        "  neg_scopes_pos_matches = re.finditer(regex_neg_pos, test_text)\n",
        "  unc_scopes_pre_matches = re.finditer(regex_unc_pre, test_text)\n",
        "  unc_scopes_pos_matches = re.finditer(regex_unc_pos, test_text)\n",
        "\n",
        "  if neg_scopes_pre_matches:\n",
        "    for match in neg_scopes_pre_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start(1)\n",
        "        end_pos = match.end(1)+1\n",
        "        #print(f\"Found '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "        predictions[id][\"NEG\"].add((start_pos,end_pos,matched_word))\n",
        "\n",
        "        # # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "\n",
        "        predictions[id][\"NSCO\"].add((sc_start_pos,sc_end_pos,scope_word))\n",
        "\n",
        "        #print(f\"Found scope '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "  \n",
        "  if neg_scopes_pos_matches:\n",
        "    for match in neg_scopes_pos_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        scope_word = match.group(1)\n",
        "        sc_start_pos = match.start()\n",
        "        sc_end_pos = match.end(1)+1\n",
        "\n",
        "\n",
        "       #print(f\"Found '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "        # # Get the scope word\n",
        "        matched_word = match.group(2)\n",
        "        start_pos = sc_end_pos\n",
        "        end_pos = match.end(2)+1\n",
        "\n",
        "\n",
        "        predictions[id][\"NEG\"].add((start_pos,end_pos,matched_word))\n",
        "        predictions[id][\"NSCO\"].add((sc_start_pos,sc_end_pos,scope_word))\n",
        "\n",
        "        #print(f\"Found scope '{match_word}' at positions ({start_pos}, {end_pos})\")\n",
        "  \n",
        "  if unc_scopes_pre_matches:\n",
        "    for match in unc_scopes_pre_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end(1)+1\n",
        "        #print(f\"Found '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "        predictions[id][\"UNC\"].add((start_pos,end_pos,matched_word))\n",
        "\n",
        "\n",
        "        # # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "        #print(f\"Found scope '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "\n",
        "        predictions[id][\"USCO\"].add((sc_start_pos,sc_end_pos,scope_word))\n",
        "    \n",
        "    if unc_scopes_pos_matches:\n",
        "      for match in unc_scopes_pos_matches:\n",
        "          #print(\"Whole match:\", match.group(0))\n",
        "          # Get the matched word and its starting/ending positions\n",
        "          scope_word = match.group(1)\n",
        "          sc_start_pos = match.start()\n",
        "          sc_end_pos = match.end(1)+1\n",
        "          #print(f\"Found '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "          # # Get the scope word\n",
        "          matched_word = match.group(2)\n",
        "          start_pos = sc_end_pos\n",
        "          end_pos = match.end(2)+1\n",
        "          #print(f\"Found scope '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "          predictions[id][\"UNC\"].add((start_pos,end_pos,matched_word))\n",
        "          predictions[id][\"USCO\"].add((sc_start_pos,sc_end_pos,scope_word))\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIXFSiPV_n-5"
      },
      "source": [
        "Sort the text predictions by starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "kP9PXuiBy5xA"
      },
      "outputs": [],
      "source": [
        "for dict in predictions:\n",
        "    for key,value in dict.items():\n",
        "\n",
        "      sorted_value=sorted(list(value), key=lambda x: x[0])\n",
        "      dict[key] = sorted_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tKQTkf9_xsz"
      },
      "source": [
        "Get ground thruth from testing_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "nsXxzCX3FXB7"
      },
      "outputs": [],
      "source": [
        "def get_gt_format(document):\n",
        "    neg_predictions, unc_predictions, nsco_predictions, usco_predictions = [], [], [], []\n",
        "    text = document[\"data\"][\"text\"]\n",
        "    for result_element in document[\"predictions\"][0][\"result\"]:\n",
        "        start = result_element[\"value\"][\"start\"]\n",
        "        end = result_element[\"value\"][\"end\"]\n",
        "        if \"NEG\" in result_element[\"value\"][\"labels\"]:\n",
        "            neg_predictions.append((start, end, text[start:end]))\n",
        "        if \"UNC\" in result_element[\"value\"][\"labels\"]:\n",
        "            unc_predictions.append((start, end, text[start:end]))\n",
        "        if \"NSCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            nsco_predictions.append((start, end, text[start:end]))\n",
        "        if \"USCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            usco_predictions.append((start, end, text[start:end]))\n",
        "\n",
        "    return neg_predictions, unc_predictions, nsco_predictions, usco_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-QWt5ZE3-D",
        "outputId": "d6fe9521-50c4-47e2-d393-303bd97de83a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'NEG': [(395, 398, 'no '),\n",
              "  (499, 505, 'niega '),\n",
              "  (1111, 1119, 'negativo'),\n",
              "  (1141, 1144, 'no '),\n",
              "  (1163, 1166, 'no '),\n",
              "  (1194, 1203, 'negativos'),\n",
              "  (2118, 2122, 'sin ')],\n",
              " 'UNC': [],\n",
              " 'NSCO': [(398, 422, 'alergias medicamentosas '),\n",
              "  (505, 521, 'habitos toxicos '),\n",
              "  (1107, 1111, 'vih '),\n",
              "  (1144, 1150, 'inmune'),\n",
              "  (1166, 1172, 'immune'),\n",
              "  (1174, 1194, 'lues vih, vhb y vhc '),\n",
              "  (2122, 2133, 'incidencias')],\n",
              " 'USCO': []}"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FORMAT : (NEG, START, END, WORD)\n",
        "def get_ground_truth(document):\n",
        "    neg_results, unc_results, nsco_results, usco_results = get_gt_format(document)\n",
        "\n",
        "    neg_results_sorted = sorted(neg_results, key=lambda x: x[0])\n",
        "    unc_results_sorted = sorted(unc_results, key=lambda x: x[0])\n",
        "    nsco_results_sorted = sorted(nsco_results, key=lambda x: x[0])\n",
        "    usco_results_sorted = sorted(usco_results, key=lambda x: x[0])\n",
        "\n",
        "    ground_truth_dict = {\"NEG\": neg_results_sorted, \"UNC\": unc_results_sorted, \"NSCO\": nsco_results_sorted, \"USCO\": usco_results_sorted}\n",
        "\n",
        "    return ground_truth_dict\n",
        "\n",
        "\n",
        "\n",
        "get_ground_truth(testing_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "a9yh7F-jE6tU"
      },
      "outputs": [],
      "source": [
        "# List of dictionaries of GT docuemnts in the test set\n",
        "ground_truths = [get_ground_truth(document) for document in testing_set]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh9oGSB2Bcii"
      },
      "source": [
        "Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "x8dBo8oiF8Ho"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions,ground_truths):\n",
        "  precision = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  recall = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  f1 = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  tp = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_predictions = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_ground_truths = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  for d1,d2 in zip(predictions,ground_truths):\n",
        "\n",
        "    #print(d1[\"UNC\"])\n",
        "    #print(d2[\"UNC\"])\n",
        "    for key in d1:\n",
        "      #print(key)\n",
        "      for elem in d1[key]:\n",
        "        for elem2 in d2[key]:\n",
        "          if abs(elem[0]-elem2[0]) <= 1 and abs(elem[1]-elem2[1]) <=1:\n",
        "            tp[key]+=1\n",
        "            break\n",
        "\n",
        "      num_of_predictions[key]+=len(d1[key])\n",
        "      num_of_ground_truths[key]+=len(d2[key])\n",
        "\n",
        "  for key in precision:\n",
        "    precision[key] = tp[key]/num_of_predictions[key]\n",
        "    recall[key] = tp[key]/num_of_ground_truths[key]\n",
        "    f1[key] = 2*precision[key]*recall[key]/(precision[key]+recall[key])\n",
        "\n",
        "\n",
        "  return precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "ugryzw7-z1T6"
      },
      "outputs": [],
      "source": [
        "precision, recall, f1 = calculate_metrics(predictions,ground_truths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-9fgOyLAJJe",
        "outputId": "1df22244-911e-462d-9848-e232507b3f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9407699901283317, 'NSCO': 0.6150049358341559, 'UNC': 0.55625, 'USCO': 0.28125}\n",
            "{'NEG': 0.8418727915194346, 'NSCO': 0.5800744878957169, 'UNC': 0.6793893129770993, 'USCO': 0.3488372093023256}\n",
            "{'NEG': 0.8885780885780885, 'NSCO': 0.5970292285577384, 'UNC': 0.6116838487972509, 'USCO': 0.3114186851211073}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': [(395, 398, 'no'), (1111, 1120, 'negativo'), (1141, 1144, 'no'), (1194, 1204, 'negativos'), (1313, 1322, 'negativo'), (2118, 2122, 'sin')], 'NSCO': [(398, 563, 'alergias medicamentosas conocidas antcededentes medico-quirurgicos: protesis mamaria, adenoidectomia niega habitos toxicos medicacio habitual anafranil25 mg/ diario'), (1107, 1111, 'vih'), (1144, 1204, 'inmune, toxoplasma no immune, lues vih, vhb y vhc negativos'), (1190, 1194, 'vhc'), (1309, 1313, 'sgb'), (2122, 2134, 'incidencias')], 'UNC': [(3460, 3466, 'puede')], 'USCO': [(3466, 3537, 'alternarse cada 4 horas con 1 comprimido de ibuprofeno 600mg si dolor)')]}\n",
            "{'NEG': [(395, 398, 'no '), (499, 505, 'niega '), (1111, 1119, 'negativo'), (1141, 1144, 'no '), (1163, 1166, 'no '), (1194, 1203, 'negativos'), (2118, 2122, 'sin ')], 'UNC': [], 'NSCO': [(398, 422, 'alergias medicamentosas '), (505, 521, 'habitos toxicos '), (1107, 1111, 'vih '), (1144, 1150, 'inmune'), (1166, 1172, 'immune'), (1174, 1194, 'lues vih, vhb y vhc '), (2122, 2133, 'incidencias')], 'USCO': []}\n"
          ]
        }
      ],
      "source": [
        "print(predictions[0])\n",
        "print(get_ground_truth(testing_set[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vV3QoBIEv04",
        "outputId": "7e781f6a-78db-4446-e372-497c236e7c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9407699901283317, 'NSCO': 0.6150049358341559, 'UNC': 0.55625, 'USCO': 0.28125}\n",
            "{'NEG': 0.8418727915194346, 'NSCO': 0.5800744878957169, 'UNC': 0.6793893129770993, 'USCO': 0.3488372093023256}\n",
            "{'NEG': 0.8885780885780885, 'NSCO': 0.5970292285577384, 'UNC': 0.6116838487972509, 'USCO': 0.3114186851211073}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiqTWLBOGVkt"
      },
      "source": [
        "CUTEXT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvBibqG3Ga_9",
        "outputId": "a38ce7ac-35f3-4b0f-a364-8e8b056788bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9407699901283317, 'NSCO': 0.6150049358341559, 'UNC': 0.55625, 'USCO': 0.28125}\n",
            "{'NEG': 0.8418727915194346, 'NSCO': 0.5800744878957169, 'UNC': 0.6793893129770993, 'USCO': 0.3488372093023256}\n",
            "{'NEG': 0.8885780885780885, 'NSCO': 0.5970292285577384, 'UNC': 0.6116838487972509, 'USCO': 0.3114186851211073}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9407699901283317, 'NSCO': 0.6150049358341559, 'UNC': 0.55625, 'USCO': 0.28125}\n",
            "{'NEG': 0.8418727915194346, 'NSCO': 0.5800744878957169, 'UNC': 0.6793893129770993, 'USCO': 0.3488372093023256}\n",
            "{'NEG': 0.8885780885780885, 'NSCO': 0.5970292285577384, 'UNC': 0.6116838487972509, 'USCO': 0.3114186851211073}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': [(395, 398, 'no'), (1111, 1120, 'negativo'), (1141, 1144, 'no'), (1194, 1204, 'negativos'), (1313, 1322, 'negativo'), (2118, 2122, 'sin')], 'NSCO': [(398, 563, 'alergias medicamentosas conocidas antcededentes medico-quirurgicos: protesis mamaria, adenoidectomia niega habitos toxicos medicacio habitual anafranil25 mg/ diario'), (1107, 1111, 'vih'), (1144, 1204, 'inmune, toxoplasma no immune, lues vih, vhb y vhc negativos'), (1190, 1194, 'vhc'), (1309, 1313, 'sgb'), (2122, 2134, 'incidencias')], 'UNC': [(3460, 3466, 'puede')], 'USCO': [(3466, 3537, 'alternarse cada 4 horas con 1 comprimido de ibuprofeno 600mg si dolor)')]}\n"
          ]
        }
      ],
      "source": [
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NEG': 0.9407699901283317, 'NSCO': 0.6150049358341559, 'UNC': 0.55625, 'USCO': 0.28125}\n",
            "{'NEG': 0.8418727915194346, 'NSCO': 0.5800744878957169, 'UNC': 0.6793893129770993, 'USCO': 0.3488372093023256}\n",
            "{'NEG': 0.8885780885780885, 'NSCO': 0.5970292285577384, 'UNC': 0.6116838487972509, 'USCO': 0.3114186851211073}\n"
          ]
        }
      ],
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

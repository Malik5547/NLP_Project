{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LkOktoy5xRL"
      },
      "source": [
        "training_set = Tot setul de date, este vectorul care contine toate documentele\n",
        "document = Element din training_set. Fiecare document contine \"data\", \"annotations\" si \"predictions\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8V6veP5-ByqK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse Training and Testing data from JSON"
      ],
      "metadata": {
        "id": "Q2GF2GPXAxXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json(file_path):\n",
        "\n",
        "  # Step 2: Open the file in read mode\n",
        "  try:\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "      # Step 3: Load the JSON data using json.load()\n",
        "      parsed_file = json.load(json_file)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  else:\n",
        "    print(\"JSON data parsed successfully!\")\n",
        "    # Step 4: Access and process the data\n",
        "    # (See examples below based on data structure)\n",
        "  return parsed_file"
      ],
      "metadata": {
        "id": "nJJ1HEr2147_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QPozyd5xRM",
        "outputId": "24f172dd-500f-4275-9bc4-fc486173abab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data parsed successfully!\n",
            "JSON data parsed successfully!\n"
          ]
        }
      ],
      "source": [
        "training_set = parse_json(\"./train_data.json\")\n",
        "testing_set = parse_json(\"./test_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_set))\n",
        "print(len(testing_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vgfvxL2s5P",
        "outputId": "efb361e6-ccd9-4482-f698-8e015209d9fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "254\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rDlG9kmj5xRO"
      },
      "outputs": [],
      "source": [
        "predictions = [document[\"predictions\"] for document in training_set]\n",
        "texts = [document[\"data\"][\"text\"] for document in training_set]\n",
        "test_texts = [document[\"data\"][\"text\"] for document in testing_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract terms given by CUTEXT"
      ],
      "metadata": {
        "id": "PO47cFyb30E-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DEnc1hI2-dcN"
      },
      "outputs": [],
      "source": [
        "def extract_terms_from_file(file_path):\n",
        "    terms = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith(\"Term:\"):\n",
        "                term = line.split(\"Term:\")[1].strip()\n",
        "                terms.append(term)\n",
        "    return terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oiUsBmvlK7BF"
      },
      "outputs": [],
      "source": [
        "def parse_terms(extracted_terms):\n",
        "  new_terms = []\n",
        "  for term in extracted_terms:\n",
        "    if term[0].isalpha() and term[-1].isalpha() and \"**\" not in term and \"(\" not in term and \")\" not in term and len(term)>3:\n",
        "      new_terms.append(term)\n",
        "  return new_terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Ur92zYTcItiX"
      },
      "outputs": [],
      "source": [
        "file_path = \"./terms_raw.txt\"\n",
        "# Extract terms from the file\n",
        "cutext_terms = extract_terms_from_file(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOcMyguOI351",
        "outputId": "de4ad4ff-0535-40d1-aae8-628c03fcbc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21554\n"
          ]
        }
      ],
      "source": [
        "print(len(cutext_terms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kk3JZb87Mt83"
      },
      "outputs": [],
      "source": [
        "cutext_terms = parse_terms(cutext_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKPRePm5xRO"
      },
      "source": [
        "Extract NEG, UNC, NSCO and USCO from Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CgMZeYPB5xRO"
      },
      "outputs": [],
      "source": [
        "# Gets a list of tuples representing character offsets and returns list of words\n",
        "def get_words(text, offsets):\n",
        "  words = []\n",
        "  for start, end in offsets:\n",
        "    #words.append(text[start:end-1])\n",
        "    #words.append(text[start-1:end])\n",
        "    if text[start-1].isalpha():\n",
        "      s=start-1\n",
        "    else:\n",
        "      s=start\n",
        "    if text[end-1].isalpha():\n",
        "      e=end\n",
        "    else:\n",
        "      e=end-1\n",
        "    words.append(text[s:e])\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NNwlbJFT5xRP"
      },
      "outputs": [],
      "source": [
        "# Parses a document and returns 4 lists of tuples representing words\n",
        "def find_cues_and_scopes(document):\n",
        "  neg_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NEG\" in result_element[\"value\"][\"labels\"]]\n",
        "  unc_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"UNC\" in result_element[\"value\"][\"labels\"]]\n",
        "  nsco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"NSCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  usco_postitions_pairs = [(result_element[\"value\"][\"start\"], result_element[\"value\"][\"end\"]) for result_element in document[\"predictions\"][0][\"result\"] if \"USCO\" in result_element[\"value\"][\"labels\"]]\n",
        "  neg_words = get_words(document[\"data\"][\"text\"], neg_postitions_pairs)\n",
        "  unc_words = get_words(document[\"data\"][\"text\"], unc_postitions_pairs)\n",
        "  nsco_words = get_words(document[\"data\"][\"text\"], nsco_postitions_pairs)\n",
        "  usco_words = get_words(document[\"data\"][\"text\"], usco_postitions_pairs)\n",
        "  return neg_words, unc_words, nsco_words, usco_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3KCH-BsM5xRP"
      },
      "outputs": [],
      "source": [
        "NEG = set()\n",
        "UNC = set()\n",
        "NSCO = set()\n",
        "USCO = set()\n",
        "\n",
        "num_nsco = 0\n",
        "num_usco = 0\n",
        "for document in training_set:\n",
        "  neg_words, unc_words, nsco_words, usco_words = find_cues_and_scopes(document)\n",
        "  nsco_words_set = set(nsco_words)\n",
        "  usco_words_set = set(usco_words)\n",
        "  num_nsco += len(nsco_words)\n",
        "  num_usco += len(usco_words)\n",
        "\n",
        "  NEG.update(neg_words)\n",
        "  UNC.update(unc_words)\n",
        "  NSCO.update(nsco_words)\n",
        "  USCO.update(usco_words)\n",
        "\n",
        "# Removing spaces and punctation signs from the start and end of each string\n",
        "NEG = {word.strip(\" ,.!?;)\") for word in NEG}\n",
        "UNC = {word.strip(\" ,.!?);\") for word in UNC}\n",
        "NSCO = {word.strip(\" ,.!?;)\") for word in NSCO}\n",
        "USCO = {word.strip(\" ,.!?);\") for word in USCO}\n",
        "\n",
        "# Remove negation from UNC\n",
        "for word in NEG:\n",
        "  if word in UNC:\n",
        "    UNC.remove(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine USCO and NSCO in SCOPE_words"
      ],
      "metadata": {
        "id": "GjomeTlu8oHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_SCOPES = NSCO.union(USCO)\n",
        "\n",
        "# A set with all individual words from the scopes\n",
        "SCOPE_words = set()         # ['erc', '(29/05/18)', 'ser', 'visibles', 'extratono', 'inicia', 'valor', 'frialdad', 'medicamentoses', 'neoformativo']\n",
        "for scope in ALL_SCOPES:\n",
        "  SCOPE_words.update(scope.split())\n",
        "\n",
        "print(\"NEG_UNC words before processing: \", len(SCOPE_words))\n",
        "\n",
        "# Remove all symbols and numbers from the set\n",
        "SCOPE_words = {word for word in SCOPE_words if word.isalpha()}\n",
        "SCOPE_words = list(SCOPE_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtv4WScpWxrl",
        "outputId": "46d62401-10c1-47ab-f3dd-30b7fd71905d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEG_UNC words before processing:  3184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine SCOPE_words with extracted_terms from CUTEXT"
      ],
      "metadata": {
        "id": "28sA1kjp9D76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_terms = list(set(cutext_terms+SCOPE_words))"
      ],
      "metadata": {
        "id": "cTOnXwCaYtMx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(extracted_terms))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKDyK0dRY929",
        "outputId": "9b8e8cfd-e3b9-4a0d-a4a9-827fe4c173e3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_vplFWHbEprg"
      },
      "outputs": [],
      "source": [
        "extracted_terms.sort(key=len,reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare REGEX"
      ],
      "metadata": {
        "id": "MeXd2xOB_OOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TIKVWGYf3fv1"
      },
      "outputs": [],
      "source": [
        "NEG_pattern = \"|\".join(NEG)\n",
        "UNC_pattern = \"|\".join(UNC)\n",
        "#SCOPE pattern with CUTEXT + NSCO+USCO\n",
        "SCOPE_pattern = \"|\".join(extracted_terms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SCOPE pattern just for CUTEXT\n",
        "SCOPE_pattern_CUTEXT = \"|\".join(cutext_terms)"
      ],
      "metadata": {
        "id": "cxmKcN5VCYZk"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PgvAtD98vpxh"
      },
      "outputs": [],
      "source": [
        "regex_neg_pre=rf\"\\b({NEG_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({SCOPE_pattern}\\b)\"\n",
        "regex_neg_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\b({NEG_pattern})\\b\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({SCOPE_pattern})\\b\"\n",
        "regex_unc_pos=rf\"\\b({SCOPE_pattern})\\b\\s\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\b({UNC_pattern})\\b\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s+((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\""
      ],
      "metadata": {
        "id": "ZvOrpfG57VM3"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex_neg_pre=rf\"\\b({NEG_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,7}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern_CUTEXT})\\b\\s*){{0,7}})\""
      ],
      "metadata": {
        "id": "e3pfd3RWEVPm"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex_neg_pre =rf\"\\b({NEG_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\"\n",
        "regex_unc_pre=rf\"\\b({UNC_pattern})\\b\\s*((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\""
      ],
      "metadata": {
        "id": "8HpQRAAxLVR-"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex_neg_pos=rf\"((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\\s*\\b({NEG_pattern})\\b\"\n",
        "regex_unc_pos=rf\"((?:\\b(?:{SCOPE_pattern})\\b\\s*){{0,5}})\\s*\\b({UNC_pattern})\\b\""
      ],
      "metadata": {
        "id": "tnaPBhtMKN_X"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(SCOPE_pattern))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5piE7K3VqNZ",
        "outputId": "8b6db2b6-ea5f-43ee-fc17-b6cdb8d2ff2d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "323583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions"
      ],
      "metadata": {
        "id": "U66u6gH3_aM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i in range(len(test_texts)):\n",
        "  dict = {\"NEG\":[],\"NSCO\":[],\"UNC\":[],\"USCO\":[]}\n",
        "  predictions.append(dict)\n",
        "\n",
        "\n",
        "for id, test_text in enumerate(test_texts):\n",
        "  neg_scopes_pre_matches = re.finditer(regex_neg_pre, test_text)\n",
        "  neg_scopes_pos_matches = re.finditer(regex_neg_pos, test_text)\n",
        "  unc_scopes_pre_matches = re.finditer(regex_unc_pre, test_text)\n",
        "  unc_scopes_pos_matches = re.finditer(regex_unc_pos, test_text)\n",
        "\n",
        "  if neg_scopes_pre_matches:\n",
        "    for match in neg_scopes_pre_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start(1)\n",
        "        end_pos = match.end(1)+1\n",
        "        #print(f\"Found '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "        predictions[id][\"NEG\"].append((start_pos,end_pos,matched_word))\n",
        "\n",
        "        # # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "\n",
        "        predictions[id][\"NSCO\"].append((sc_start_pos,sc_end_pos,scope_word))\n",
        "\n",
        "        #print(f\"Found scope '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "  '''\n",
        "  if neg_scopes_pos_matches:\n",
        "    for match in neg_scopes_pos_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        scope_word = match.group(1)\n",
        "        sc_start_pos = match.start()\n",
        "        sc_end_pos = match.end(1)+1\n",
        "\n",
        "\n",
        "       #print(f\"Found '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "        # # Get the scope word\n",
        "        matched_word = match.group(2)\n",
        "        start_pos = sc_end_pos\n",
        "        end_pos = match.end(2)+1\n",
        "\n",
        "\n",
        "        predictions[id][\"NEG\"].append((start_pos,end_pos,matched_word))\n",
        "        predictions[id][\"NSCO\"].append((sc_start_pos,sc_end_pos,scope_word))\n",
        "\n",
        "        #print(f\"Found scope '{match_word}' at positions ({start_pos}, {end_pos})\")\n",
        "  '''\n",
        "  if unc_scopes_pre_matches:\n",
        "    for match in unc_scopes_pre_matches:\n",
        "        #print(\"Whole match:\", match.group(0))\n",
        "        # Get the matched word and its starting/ending positions\n",
        "        matched_word = match.group(1)\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end(1)+1\n",
        "        #print(f\"Found '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "        predictions[id][\"UNC\"].append((start_pos,end_pos,matched_word))\n",
        "\n",
        "\n",
        "        # # Get the scope word\n",
        "        scope_word = match.group(2)\n",
        "        sc_start_pos = end_pos\n",
        "        sc_end_pos = match.end(2)+1\n",
        "        #print(f\"Found scope '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "\n",
        "        predictions[id][\"USCO\"].append((sc_start_pos,sc_end_pos,scope_word))\n",
        "    '''\n",
        "    if unc_scopes_pos_matches:\n",
        "      for match in unc_scopes_pos_matches:\n",
        "          #print(\"Whole match:\", match.group(0))\n",
        "          # Get the matched word and its starting/ending positions\n",
        "          scope_word = match.group(1)\n",
        "          sc_start_pos = match.start()\n",
        "          sc_end_pos = match.end(1)+1\n",
        "          #print(f\"Found '{scope_word}' at positions ({sc_start_pos}, {sc_end_pos})\")\n",
        "          # # Get the scope word\n",
        "          matched_word = match.group(2)\n",
        "          start_pos = sc_end_pos\n",
        "          end_pos = match.end(2)+1\n",
        "          #print(f\"Found scope '{matched_word}' at positions ({start_pos}, {end_pos})\")\n",
        "\n",
        "          predictions[id][\"UNC\"].append((start_pos,end_pos,matched_word))\n",
        "          predictions[id][\"USCO\"].append((sc_start_pos,sc_end_pos,scope_word))\n",
        "      '''"
      ],
      "metadata": {
        "id": "ERlHYxLZvIHG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort the text predictions by starting point"
      ],
      "metadata": {
        "id": "XIXFSiPV_n-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dict in predictions:\n",
        "    for key,value in dict.items():\n",
        "\n",
        "      sorted_value=sorted(value, key=lambda x: x[0])\n",
        "      dict[key] = sorted_value"
      ],
      "metadata": {
        "id": "kP9PXuiBy5xA"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get ground thruth from testing_set"
      ],
      "metadata": {
        "id": "0tKQTkf9_xsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gt_format(document):\n",
        "    neg_predictions, unc_predictions, nsco_predictions, usco_predictions = [], [], [], []\n",
        "    text = document[\"data\"][\"text\"]\n",
        "    for result_element in document[\"predictions\"][0][\"result\"]:\n",
        "        start = result_element[\"value\"][\"start\"]\n",
        "        end = result_element[\"value\"][\"end\"]\n",
        "        if \"NEG\" in result_element[\"value\"][\"labels\"]:\n",
        "            neg_predictions.append((start, end, text[start:end]))\n",
        "        if \"UNC\" in result_element[\"value\"][\"labels\"]:\n",
        "            unc_predictions.append((start, end, text[start:end]))\n",
        "        if \"NSCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            nsco_predictions.append((start, end, text[start:end]))\n",
        "        if \"USCO\" in result_element[\"value\"][\"labels\"]:\n",
        "            usco_predictions.append((start, end, text[start:end]))\n",
        "\n",
        "    return neg_predictions, unc_predictions, nsco_predictions, usco_predictions"
      ],
      "metadata": {
        "id": "nsXxzCX3FXB7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORMAT : (NEG, START, END, WORD)\n",
        "def get_ground_truth(document):\n",
        "    neg_results, unc_results, nsco_results, usco_results = get_gt_format(document)\n",
        "\n",
        "    neg_results_sorted = sorted(neg_results, key=lambda x: x[0])\n",
        "    unc_results_sorted = sorted(unc_results, key=lambda x: x[0])\n",
        "    nsco_results_sorted = sorted(nsco_results, key=lambda x: x[0])\n",
        "    usco_results_sorted = sorted(usco_results, key=lambda x: x[0])\n",
        "\n",
        "    ground_truth_dict = {\"NEG\": neg_results_sorted, \"UNC\": unc_results_sorted, \"NSCO\": nsco_results_sorted, \"USCO\": usco_results_sorted}\n",
        "\n",
        "    return ground_truth_dict\n",
        "\n",
        "\n",
        "\n",
        "get_ground_truth(testing_set[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-QWt5ZE3-D",
        "outputId": "d6fe9521-50c4-47e2-d393-303bd97de83a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NEG': [(395, 398, 'no '),\n",
              "  (499, 505, 'niega '),\n",
              "  (1111, 1119, 'negativo'),\n",
              "  (1141, 1144, 'no '),\n",
              "  (1163, 1166, 'no '),\n",
              "  (1194, 1203, 'negativos'),\n",
              "  (2118, 2122, 'sin ')],\n",
              " 'UNC': [],\n",
              " 'NSCO': [(398, 422, 'alergias medicamentosas '),\n",
              "  (505, 521, 'habitos toxicos '),\n",
              "  (1107, 1111, 'vih '),\n",
              "  (1144, 1150, 'inmune'),\n",
              "  (1166, 1172, 'immune'),\n",
              "  (1174, 1194, 'lues vih, vhb y vhc '),\n",
              "  (2122, 2133, 'incidencias')],\n",
              " 'USCO': []}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of dictionaries of GT docuemnts in the test set\n",
        "ground_truths = [get_ground_truth(document) for document in testing_set]\n"
      ],
      "metadata": {
        "id": "a9yh7F-jE6tU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Metrics"
      ],
      "metadata": {
        "id": "Bh9oGSB2Bcii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(predictions,ground_truths):\n",
        "  precision = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  recall = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  f1 = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  tp = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_predictions = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  num_of_ground_truths = {\"NEG\":0,\"NSCO\":0,\"UNC\":0,\"USCO\":0}\n",
        "  for d1,d2 in zip(predictions,ground_truths):\n",
        "\n",
        "    #print(d1[\"UNC\"])\n",
        "    #print(d2[\"UNC\"])\n",
        "    for key in d1:\n",
        "      #print(key)\n",
        "      for elem in d1[key]:\n",
        "        for elem2 in d2[key]:\n",
        "          if abs(elem[0]-elem2[0]) <= 1 and abs(elem[1]-elem2[1]) <=1:\n",
        "            tp[key]+=1\n",
        "            break\n",
        "\n",
        "      num_of_predictions[key]+=len(d1[key])\n",
        "      num_of_ground_truths[key]+=len(d2[key])\n",
        "\n",
        "  for key in precision:\n",
        "    precision[key] = tp[key]/num_of_predictions[key]\n",
        "    recall[key] = tp[key]/num_of_ground_truths[key]\n",
        "    f1[key] = 2*precision[key]*recall[key]/(precision[key]+recall[key])\n",
        "\n",
        "\n",
        "  return precision, recall, f1\n"
      ],
      "metadata": {
        "id": "x8dBo8oiF8Ho"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1 = calculate_metrics(predictions,ground_truths)"
      ],
      "metadata": {
        "id": "ugryzw7-z1T6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-9fgOyLAJJe",
        "outputId": "1df22244-911e-462d-9848-e232507b3f87"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEG': 0.924314096499527, 'NSCO': 0.5799432355723746, 'UNC': 0.5963855421686747, 'USCO': 0.22289156626506024}\n",
            "{'NEG': 0.8630742049469965, 'NSCO': 0.5707635009310987, 'UNC': 0.7557251908396947, 'USCO': 0.2868217054263566}\n",
            "{'NEG': 0.8926450433988123, 'NSCO': 0.5753167526982637, 'UNC': 0.6666666666666667, 'USCO': 0.2508474576271187}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vV3QoBIEv04",
        "outputId": "7e781f6a-78db-4446-e372-497c236e7c5d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEG': 0.9249578414839797, 'NSCO': 0.5168634064080945, 'UNC': 0.5706214689265536, 'USCO': 0.20903954802259886}\n",
            "{'NEG': 0.9690812720848057, 'NSCO': 0.5707635009310987, 'UNC': 0.7709923664122137, 'USCO': 0.2868217054263566}\n",
            "{'NEG': 0.9465056082830027, 'NSCO': 0.5424778761061947, 'UNC': 0.6558441558441559, 'USCO': 0.2418300653594771}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUTEXT\n"
      ],
      "metadata": {
        "id": "aiqTWLBOGVkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision)\n",
        "print(recall)\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvBibqG3Ga_9",
        "outputId": "a38ce7ac-35f3-4b0f-a364-8e8b056788bd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEG': 0.9226289517470881, 'NSCO': 0.15224625623960067, 'UNC': 0.5611111111111111, 'USCO': 0.07777777777777778}\n",
            "{'NEG': 0.9796819787985865, 'NSCO': 0.17039106145251395, 'UNC': 0.7709923664122137, 'USCO': 0.10852713178294573}\n",
            "{'NEG': 0.950299914310197, 'NSCO': 0.16080843585237256, 'UNC': 0.6495176848874598, 'USCO': 0.09061488673139159}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}